{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOms+/IAeE6BusKgwDfmHf+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kucan23/Dark-Pattern_ML/blob/main/keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjBXVY97Hnhm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5482b22-09d9-4b36-8412-15e793724270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "Successfully installed tensorflow-2.15.0.post1\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWeaqY6giazE",
        "outputId": "8b7092fe-2139-4bc5-8805-e32fba40f1bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVPldooYiaz6",
        "outputId": "163497f0-95ba-483e-d303-3e7f5089ca0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "from plotly.offline import iplot\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dropout\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from keras.regularizers import l2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wgRp8LAia3f",
        "outputId": "024ecdc3-d19b-47bf-ca0c-d9a96bedbe90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"text_data.tsv\"\n",
        "df = pd.read_csv(file_path, sep='\\t', header=0)\n",
        "\n",
        "target_variable = 'Pattern Category'\n",
        "data = df[\"text\"]\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "LiWe3h2ria4N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a0e8f3-62fa-4a02-a4e7-5ac10b486d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2356, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[target_variable].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFukCjLCia8Y",
        "outputId": "661b9eb0-a83c-4453-e150-a36cb3cc07c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Not Dark Pattern    1178\n",
              "Scarcity             418\n",
              "Social Proof         312\n",
              "Urgency              210\n",
              "Misdirection         195\n",
              "Obstruction           27\n",
              "Sneaking              12\n",
              "Forced Action          4\n",
              "Name: Pattern Category, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4Mp-H2Zhzau",
        "outputId": "7dccbd94-be5d-44f8-a661-80b4a803c87d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Not Dark Pattern    1178\n",
              "Scarcity             418\n",
              "Social Proof         312\n",
              "Urgency              210\n",
              "Misdirection         195\n",
              "Obstruction           27\n",
              "Sneaking              12\n",
              "Forced Action          4\n",
              "Name: Pattern Category, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"text\"] = df[\"text\"].reset_index(drop=True)\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "    text = BAD_SYMBOLS_RE.sub('', text)\n",
        "\n",
        "    text = re.sub(r'\\W+', '', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
        "    return text\n",
        "df[\"text\"] = df[\"text\"].apply(clean_text)"
      ],
      "metadata": {
        "id": "9emg2Uvvia9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MAX_NB_WORDS = 3000\n",
        "\n",
        "# MAX_SEQUENCE_LENGTH = 2000\n",
        "MAX_SEQUENCE_LENGTH = 500\n",
        "\n",
        "# EMBEDDING_DIM = 200\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(df[\"text\"])\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0EvkrqgibBa",
        "outputId": "c201d717-fab0-4696-d74d-d72f70e5eacf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3871 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = tokenizer.texts_to_sequences(df[\"text\"].values)\n",
        "\n",
        "print(X[0:2])\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H85UUH_bibCT",
        "outputId": "8e18d2de-7256-4b5c-b268-470ab55086c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[286, 33, 8, 3, 38], [1386, 1387]]\n",
            "Shape of data tensor: (2356, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# le = LabelEncoder()\n",
        "# Y = le.fit_transform(df[target_variable])\n",
        "\n",
        "# print('Shape of label tensor:', Y.shape)\n",
        "# print(le.classes_)\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "Y = pd.get_dummies(df[target_variable])\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbX0oX1yl8g4",
        "outputId": "6ac31297-0d91-4bab-c3ab-3ea0c32f7c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2356, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hRJPpSBNiDvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 2)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afd9SAALl8iL",
        "outputId": "43f28e46-b8b5-40e7-c61d-242cb810b559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1884, 500) (1884, 8)\n",
            "(472, 500) (472, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_sums = np.sum(Y_test, axis=0)\n",
        "\n",
        "print(column_sums)\n",
        "\n",
        "\n",
        "column_sums = np.sum(Y_train, axis=0)\n",
        "\n",
        "print(column_sums)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMJD5xn9pdJe",
        "outputId": "cee0e422-fab1-4c6f-f788-5dabf277e121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forced Action         1\n",
            "Misdirection         44\n",
            "Not Dark Pattern    220\n",
            "Obstruction           6\n",
            "Scarcity             86\n",
            "Sneaking              1\n",
            "Social Proof         71\n",
            "Urgency              43\n",
            "dtype: int64\n",
            "Forced Action         3\n",
            "Misdirection        151\n",
            "Not Dark Pattern    958\n",
            "Obstruction          21\n",
            "Scarcity            332\n",
            "Sneaking             11\n",
            "Social Proof        241\n",
            "Urgency             167\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pcflgOEmpdTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "# model.add(Dense(50, activation='relu'))\n",
        "model.add(LSTM(100, dropout=0.4, recurrent_dropout=0.2))\n",
        "# model.add(LSTM(100, batch_size=1, return_sequences=True))\n",
        "model.add(Dense(8, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0005), metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsr9AUOCl8lF",
        "outputId": "b419b24d-ebd9-4698-c653-4a0f7b825c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 500, 100)          300000    \n",
            "                                                                 \n",
            " spatial_dropout1d_4 (Spati  (None, 500, 100)          0         \n",
            " alDropout1D)                                                    \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 100)               80400     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 8)                 808       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 381208 (1.45 MB)\n",
            "Trainable params: 381208 (1.45 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "batch_size =64\n",
        "early_stopping = EarlyStopping( monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y55J_n6xl8mL",
        "outputId": "f5012021-5b82-4e52-c8a5-983eac72e44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "24/24 [==============================] - 24s 852ms/step - loss: 1.8072 - accuracy: 0.4565 - val_loss: 1.4367 - val_accuracy: 0.5225\n",
            "Epoch 2/20\n",
            "24/24 [==============================] - 20s 854ms/step - loss: 1.3913 - accuracy: 0.5050 - val_loss: 1.3622 - val_accuracy: 0.5225\n",
            "Epoch 3/20\n",
            "24/24 [==============================] - 20s 819ms/step - loss: 1.3220 - accuracy: 0.5050 - val_loss: 1.2573 - val_accuracy: 0.5225\n",
            "Epoch 4/20\n",
            "24/24 [==============================] - 20s 825ms/step - loss: 1.1934 - accuracy: 0.5481 - val_loss: 1.1043 - val_accuracy: 0.6313\n",
            "Epoch 5/20\n",
            "24/24 [==============================] - 20s 817ms/step - loss: 0.9772 - accuracy: 0.6722 - val_loss: 0.9070 - val_accuracy: 0.6472\n",
            "Epoch 6/20\n",
            "24/24 [==============================] - 21s 880ms/step - loss: 0.7509 - accuracy: 0.7910 - val_loss: 0.6844 - val_accuracy: 0.8408\n",
            "Epoch 7/20\n",
            "24/24 [==============================] - 20s 844ms/step - loss: 0.5316 - accuracy: 0.8746 - val_loss: 0.5316 - val_accuracy: 0.8647\n",
            "Epoch 8/20\n",
            "24/24 [==============================] - 21s 863ms/step - loss: 0.3954 - accuracy: 0.9111 - val_loss: 0.4399 - val_accuracy: 0.9045\n",
            "Epoch 9/20\n",
            "24/24 [==============================] - 19s 807ms/step - loss: 0.3027 - accuracy: 0.9310 - val_loss: 0.4023 - val_accuracy: 0.9098\n",
            "Epoch 10/20\n",
            "24/24 [==============================] - 20s 839ms/step - loss: 0.2438 - accuracy: 0.9456 - val_loss: 0.3495 - val_accuracy: 0.9125\n",
            "Epoch 11/20\n",
            "24/24 [==============================] - 21s 869ms/step - loss: 0.2203 - accuracy: 0.9536 - val_loss: 0.3347 - val_accuracy: 0.9178\n",
            "Epoch 12/20\n",
            "24/24 [==============================] - 19s 788ms/step - loss: 0.1776 - accuracy: 0.9562 - val_loss: 0.3230 - val_accuracy: 0.9257\n",
            "Epoch 13/20\n",
            "24/24 [==============================] - 20s 843ms/step - loss: 0.1655 - accuracy: 0.9648 - val_loss: 0.3112 - val_accuracy: 0.9310\n",
            "Epoch 14/20\n",
            "24/24 [==============================] - 20s 849ms/step - loss: 0.1452 - accuracy: 0.9642 - val_loss: 0.3009 - val_accuracy: 0.9337\n",
            "Epoch 15/20\n",
            "24/24 [==============================] - 19s 813ms/step - loss: 0.1352 - accuracy: 0.9675 - val_loss: 0.2918 - val_accuracy: 0.9390\n",
            "Epoch 16/20\n",
            "24/24 [==============================] - 19s 788ms/step - loss: 0.1211 - accuracy: 0.9681 - val_loss: 0.3044 - val_accuracy: 0.9231\n",
            "Epoch 17/20\n",
            "24/24 [==============================] - 20s 847ms/step - loss: 0.1102 - accuracy: 0.9695 - val_loss: 0.3302 - val_accuracy: 0.9151\n",
            "Epoch 18/20\n",
            "24/24 [==============================] - 20s 840ms/step - loss: 0.1077 - accuracy: 0.9721 - val_loss: 0.2852 - val_accuracy: 0.9416\n",
            "Epoch 19/20\n",
            "24/24 [==============================] - 19s 812ms/step - loss: 0.0947 - accuracy: 0.9794 - val_loss: 0.2895 - val_accuracy: 0.9363\n",
            "Epoch 20/20\n",
            "24/24 [==============================] - 20s 822ms/step - loss: 0.0917 - accuracy: 0.9761 - val_loss: 0.2844 - val_accuracy: 0.9337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = model.predict(X_train)\n",
        "print(y_pred[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s8wGM--e6tH",
        "outputId": "0e5f688b-65ce-4c47-dd8b-dd88d6afc3ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59/59 [==============================] - 6s 105ms/step\n",
            "[5.8725236e-06 4.7144908e-04 9.9725145e-01 6.6637194e-06 1.1414397e-03\n",
            " 2.1669828e-05 3.9400905e-04 7.0750248e-04]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_t=tf.argmax(Y_train, axis=1)\n",
        "\n",
        "training_accuracy = accuracy_score(y_t, y_pred_classes)\n",
        "print(\"Training Accuracy:\", training_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC-tJj-3MFxY",
        "outputId": "4ab68667-01f6-412e-c63a-c7985d13c066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.9697452229299363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accr = model.evaluate(X_test,Y_test)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ss9sNAqHl8pM",
        "outputId": "69691908-0031-499e-cece-38cbe49dbb0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 1s 85ms/step - loss: 0.2991 - accuracy: 0.9322\n",
            "Test set\n",
            "  Loss: 0.299\n",
            "  Accuracy: 0.932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(Y_test.shape)\n",
        "\n",
        "y_r=tf.argmax(Y_test, axis=1)"
      ],
      "metadata": {
        "id": "7JxW_9Aw0TJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e26642a6-493b-4bc2-b0f1-3e2103263ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(472, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_zMioWe8sXBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "y_proba = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_proba, axis=1)\n",
        "\n",
        "# Confusion matrix\n",
        "confusion_mat = confusion_matrix(y_r, y_pred_classes)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_r, y_pred_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ErncRWUw-Ln",
        "outputId": "d7a85a21-67b8-4feb-f270-b5e4ff867c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 1s 84ms/step\n",
            "Confusion Matrix:\n",
            "[[  0   1   0   0   0   0   0   0]\n",
            " [  0  31   8   0   1   0   1   3]\n",
            " [  0   2 215   0   1   0   1   1]\n",
            " [  0   0   0   6   0   0   0   0]\n",
            " [  0   0   1   0  85   0   0   0]\n",
            " [  0   0   1   0   0   0   0   0]\n",
            " [  0   0   5   0   0   0  65   1]\n",
            " [  0   0   5   0   0   0   0  38]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.91      0.70      0.79        44\n",
            "           2       0.91      0.98      0.95       220\n",
            "           3       1.00      1.00      1.00         6\n",
            "           4       0.98      0.99      0.98        86\n",
            "           5       0.00      0.00      0.00         1\n",
            "           6       0.97      0.92      0.94        71\n",
            "           7       0.88      0.88      0.88        43\n",
            "\n",
            "    accuracy                           0.93       472\n",
            "   macro avg       0.71      0.68      0.69       472\n",
            "weighted avg       0.93      0.93      0.93       472\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:\n",
            "\n",
            "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:\n",
            "\n",
            "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:\n",
            "\n",
            "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RPWai28z3hs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"No Thanks, I don't like exclusive deals\"\n",
        "new_text = clean_text(new_text)  # Apply preprocessing\n",
        "tokenized_text = tokenizer.texts_to_sequences([new_text])[0]  # Tokenize\n",
        "input_data = pad_sequences([tokenized_text], maxlen=MAX_SEQUENCE_LENGTH)  # Pad\n",
        "\n",
        "prediction = model.predict(input_data)\n",
        "print(prediction)\n",
        "predicted_category = prediction.argmax()\n",
        "print (predicted_category)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3eRdZmKl8tN",
        "outputId": "2f89e2c1-74d9-4722-9e46-693381eccdad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 280ms/step\n",
            "[[3.7831833e-04 9.8815829e-01 3.4373032e-04 1.1437229e-03 2.5367499e-03\n",
            "  8.5703097e-04 1.8184625e-03 4.7637220e-03]]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import lite\n",
        "\n",
        "converter = lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.experimental_new_converter=True\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "\n",
        "tfmodel = converter.convert()\n",
        "open('Keras93_c.tflite', 'wb').write(tfmodel)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "KGm7KbTBl8uU",
        "outputId": "990612cd-6228-498c-8e12-6b8929d91bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Unable to load model. Filepath is not an hdf5 file (or h5py is not available) or SavedModel. Received: filepath=<keras.src.engine.sequential.Sequential object at 0x7bf6340e0910>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-9984652cc924>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Load your Keras model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Specify desired TFLite GPU version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    257\u001b[0m                         )\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     raise IOError(\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;34m\"Unable to load model. Filepath is not an hdf5 file (or h5py is not \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;34mf\"available) or SavedModel. Received: filepath={filepath}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to load model. Filepath is not an hdf5 file (or h5py is not available) or SavedModel. Received: filepath=<keras.src.engine.sequential.Sequential object at 0x7bf6340e0910>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# interpreter = tf.lite.Interpreter(model_path=\"KFINAL_c.tflite\")\n",
        "\n",
        "# interpreter.allocate_tensors()\n",
        "# input_details = interpreter.get_input_details()\n",
        "# output_details = interpreter.get_output_details()"
      ],
      "metadata": {
        "id": "4qmqBtM5l8yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new_text = \"No Thanks, I don't like exclusive dealS\"\n",
        "# new_text = clean_text(new_text)  # Apply preprocessing\n",
        "# tokenized_text = tokenizer.texts_to_sequences([new_text])[0]  # Tokenize\n",
        "# input_data = pad_sequences([tokenized_text], maxlen=MAX_SEQUENCE_LENGTH)  # Pad\n"
      ],
      "metadata": {
        "id": "RrY7LdM6r0ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_data = input_data.astype('float32')  # Convert to FLOAT32\n",
        "# interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "# interpreter.invoke()\n",
        "# output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "# predicted_category = output_data.argmax()  # Get the index of the predicted category\n",
        "# print (output_data)\n",
        "# print(predicted_category)"
      ],
      "metadata": {
        "id": "rRjfmhtRr1Y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Aby4Dsgr1aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MRZpCgBnr1eG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}